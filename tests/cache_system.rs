//! ç¼“å­˜ç³»ç»Ÿé›†æˆæµ‹è¯•
//!
//! æµ‹è¯•ç¼“å­˜çš„å¤šå±‚æ¶æ„å’ŒæŒä¹…åŒ–åŠŸèƒ½

use std::time::Duration;
use std::sync::Arc;

use monolith::translation::storage::cache::{
    CacheManager, CacheConfig, CacheKey, CacheEntry, CacheStats
};

mod common {
    include!("common/mod.rs");
}

use common::{
    TestEnvironment, TestConfigBuilder, TestDataGenerator,
    PerformanceHelper, AssertionHelper
};

/// æµ‹è¯•åŸºæœ¬ç¼“å­˜æ“ä½œ
#[tokio::test]
async fn test_basic_cache_operations() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 100,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // æµ‹è¯•åŸºæœ¬çš„å­˜å‚¨å’Œæ£€ç´¢
    let key = CacheKey::new(
        "Hello world".to_string(),
        "en".to_string(),
        "zh".to_string(),
    );
    
    let entry = CacheEntry::new(
        "Hello world".to_string(),
        "ä½ å¥½ä¸–ç•Œ".to_string(),
        Some(Duration::from_secs(300)),
    );
    
    // åˆå§‹çŠ¶æ€åº”è¯¥ä¸ºç©º
    let initial_result = cache_manager.get(&key).await
        .expect("Cache get should not error");
    assert!(initial_result.is_none(), "Cache should be empty initially");
    
    // å­˜å‚¨æ¡ç›®
    cache_manager.put(key.clone(), entry.clone()).await
        .expect("Cache put should succeed");
    
    // æ£€ç´¢æ¡ç›®
    let retrieved = cache_manager.get(&key).await
        .expect("Cache get should not error")
        .expect("Cache item should be found");
    
    assert_eq!(retrieved.original_text, entry.original_text);
    assert_eq!(retrieved.translated_text, entry.translated_text);
    
    println!("âœ… Basic cache operations test passed");
}

/// æµ‹è¯•ç¼“å­˜ç»Ÿè®¡å’Œæ€§èƒ½ç›‘æ§
#[tokio::test]
async fn test_cache_statistics_monitoring() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 50,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // åˆå§‹ç»Ÿè®¡
    let initial_stats = cache_manager.get_stats();
    assert_eq!(initial_stats.local_hits, 0);
    assert_eq!(initial_stats.misses, 0);
    
    // ç”Ÿæˆæµ‹è¯•æ•°æ®
    let test_entries = TestDataGenerator::create_test_cache_entries(10);
    
    // æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œå¹¶ç›‘æ§ç»Ÿè®¡
    for (key, entry) in &test_entries {
        // é¦–æ¬¡è®¿é—®åº”è¯¥æ˜¯miss
        let _miss_result = cache_manager.get(key).await.unwrap();
        
        // å­˜å‚¨æ¡ç›®
        cache_manager.put(key.clone(), entry.clone()).await.unwrap();
        
        // å†æ¬¡è®¿é—®åº”è¯¥æ˜¯hit
        let _hit_result = cache_manager.get(key).await.unwrap();
    }
    
    // éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    let final_stats = cache_manager.get_stats();
    assert_eq!(final_stats.misses, test_entries.len());
    assert_eq!(final_stats.local_hits, test_entries.len());
    assert!(final_stats.hit_rate() > 0.0);
    
    // æµ‹è¯•ç»Ÿè®¡é‡ç½®
    cache_manager.reset_stats();
    let reset_stats = cache_manager.get_stats();
    assert_eq!(reset_stats.local_hits, 0);
    assert_eq!(reset_stats.misses, 0);
    
    println!("âœ… Cache statistics monitoring test passed - tracked {} operations", 
             test_entries.len() * 2);
}

/// æµ‹è¯•ç¼“å­˜å®¹é‡é™åˆ¶å’ŒLRUæ·˜æ±°
#[tokio::test]
async fn test_cache_capacity_and_lru_eviction() {
    let small_capacity = 5;
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: small_capacity,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // åˆ›å»ºè¶…è¿‡å®¹é‡çš„æµ‹è¯•æ¡ç›®
    let test_entries = TestDataGenerator::create_test_cache_entries(small_capacity * 2);
    
    // å¡«å……ç¼“å­˜è¶…è¿‡å®¹é‡
    for (key, entry) in &test_entries {
        cache_manager.put(key.clone(), entry.clone()).await.unwrap();
    }
    
    // éªŒè¯ç¼“å­˜å¤§å°ä¸è¶…è¿‡é™åˆ¶
    let stats = cache_manager.get_stats();
    assert!(stats.total_entries <= small_capacity, 
           "Cache size {} should not exceed capacity {}", 
           stats.total_entries, small_capacity);
    
    // éªŒè¯æœ€è¿‘ä½¿ç”¨çš„é¡¹ç›®ä»åœ¨ç¼“å­˜ä¸­
    let recent_keys = &test_entries[test_entries.len() - small_capacity..];
    for (key, _) in recent_keys {
        let result = cache_manager.get(key).await.unwrap();
        assert!(result.is_some(), "Recently added item should still be in cache");
    }
    
    // éªŒè¯æ—©æœŸçš„é¡¹ç›®å¯èƒ½å·²è¢«æ·˜æ±°
    let early_keys = &test_entries[..test_entries.len() - small_capacity];
    let mut evicted_count = 0;
    for (key, _) in early_keys {
        let result = cache_manager.get(key).await.unwrap();
        if result.is_none() {
            evicted_count += 1;
        }
    }
    
    assert!(evicted_count > 0, "Some early items should have been evicted");
    
    println!("âœ… Cache capacity and LRU eviction test passed - evicted {} items", 
             evicted_count);
}

/// æµ‹è¯•ç¼“å­˜è¿‡æœŸæœºåˆ¶
#[tokio::test]
async fn test_cache_expiration_mechanism() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 100,
        default_ttl: Duration::from_millis(100), // çŸ­è¿‡æœŸæ—¶é—´
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // åˆ›å»ºçŸ­è¿‡æœŸæ—¶é—´çš„æ¡ç›®
    let key = CacheKey::new(
        "Expiring item".to_string(),
        "en".to_string(),
        "zh".to_string(),
    );
    
    let entry = CacheEntry::new(
        "Expiring item".to_string(),
        "è¿‡æœŸé¡¹ç›®".to_string(),
        Some(Duration::from_millis(50)), // éå¸¸çŸ­çš„TTL
    );
    
    // å­˜å‚¨æ¡ç›®
    cache_manager.put(key.clone(), entry.clone()).await.unwrap();
    
    // ç«‹å³è®¿é—®åº”è¯¥æˆåŠŸ
    let immediate_result = cache_manager.get(&key).await.unwrap();
    assert!(immediate_result.is_some(), "Item should be available immediately");
    
    // ç­‰å¾…è¿‡æœŸ
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // è®¿é—®è¿‡æœŸé¡¹ç›®
    let expired_result = cache_manager.get(&key).await.unwrap();
    assert!(expired_result.is_none(), "Expired item should not be available");
    
    // æµ‹è¯•æ¸…ç†è¿‡æœŸé¡¹ç›®
    let cleanup_count = cache_manager.cleanup_expired().await.unwrap();
    assert!(cleanup_count >= 0, "Cleanup should return non-negative count");
    
    println!("âœ… Cache expiration mechanism test passed - cleaned up {} expired items", 
             cleanup_count);
}

/// æµ‹è¯•é«˜å¹¶å‘ç¼“å­˜è®¿é—®
#[tokio::test]
async fn test_high_concurrency_cache_access() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 200,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = Arc::new(CacheManager::new(config));
    
    // å¹¶å‘æ“ä½œå‚æ•°
    let num_concurrent_tasks = 50;
    let operations_per_task = 10;
    
    // åˆ›å»ºå¹¶å‘ä»»åŠ¡
    let mut handles = vec![];
    
    for task_id in 0..num_concurrent_tasks {
        let cm = Arc::clone(&cache_manager);
        
        let handle = tokio::spawn(async move {
            let mut local_operations = 0;
            
            for op_id in 0..operations_per_task {
                let key = CacheKey::new(
                    format!("concurrent_item_{}_{}", task_id, op_id),
                    "en".to_string(),
                    "zh".to_string(),
                );
                
                let entry = CacheEntry::new(
                    format!("Item {} {}", task_id, op_id),
                    format!("é¡¹ç›® {} {}", task_id, op_id),
                    Some(Duration::from_secs(300)),
                );
                
                // å¹¶å‘å­˜å‚¨
                cm.put(key.clone(), entry).await.unwrap();
                
                // å¹¶å‘æ£€ç´¢
                let _retrieved = cm.get(&key).await.unwrap();
                
                local_operations += 1;
            }
            
            local_operations
        });
        
        handles.push(handle);
    }
    
    // ç­‰å¾…æ‰€æœ‰å¹¶å‘ä»»åŠ¡å®Œæˆ
    let (total_operations, concurrent_time) = PerformanceHelper::measure_async_time(|| async {
        let mut total = 0;
        for handle in handles {
            let ops = handle.await.expect("Concurrent task should complete");
            total += ops;
        }
        total
    }).await;
    
    // éªŒè¯å¹¶å‘æ“ä½œç»“æœ
    let expected_operations = num_concurrent_tasks * operations_per_task;
    assert_eq!(total_operations, expected_operations, 
              "All concurrent operations should complete");
    
    // éªŒè¯ç¼“å­˜çŠ¶æ€
    let final_stats = cache_manager.get_stats();
    assert!(final_stats.total_entries > 0, "Concurrent operations should populate cache");
    
    // éªŒè¯æ€§èƒ½
    let ops_per_second = total_operations as f64 / concurrent_time.as_secs_f64();
    assert!(ops_per_second > 100.0, 
           "Should achieve reasonable throughput: {} ops/sec", ops_per_second);
    
    println!("âœ… High concurrency test passed - {} operations in {:?} at {:.1} ops/sec", 
             total_operations, concurrent_time, ops_per_second);
}

/// æµ‹è¯•ç¼“å­˜é”®çš„å­—ç¬¦ä¸²è¡¨ç¤ºå’Œå“ˆå¸Œ
#[tokio::test]
async fn test_cache_key_handling() {
    let cache_manager = CacheManager::new(CacheConfig::default());
    
    // æµ‹è¯•ä¸åŒç±»å‹çš„ç¼“å­˜é”®
    let test_cases = vec![
        ("Hello world", "en", "zh"),
        ("Complex text with symbols: !@#$%^&*()", "en", "zh"),
        ("", "en", "zh"), // ç©ºæ–‡æœ¬
        ("Text", "", "zh"), // ç©ºæºè¯­è¨€
        ("Text", "en", ""), // ç©ºç›®æ ‡è¯­è¨€
        ("ä½ å¥½ä¸–ç•Œ", "zh", "en"), // ä¸­æ–‡æ–‡æœ¬
        ("Mixed ä¸­è‹±æ–‡ text", "auto", "zh"), // æ··åˆè¯­è¨€
        ("ğŸš€ Emoji test ğŸ‰", "en", "zh"), // åŒ…å«emoji
    ];
    
    for (text, src_lang, target_lang) in &test_cases {
        let key = CacheKey::new(
            text.to_string(),
            src_lang.to_string(),
            target_lang.to_string(),
        );
        
        let entry = CacheEntry::new(
            text.to_string(),
            format!("translated_{}", text),
            Some(Duration::from_secs(300)),
        );
        
        // å­˜å‚¨å’Œæ£€ç´¢åº”è¯¥éƒ½æˆåŠŸ
        let store_result = cache_manager.put(key.clone(), entry.clone()).await;
        assert!(store_result.is_ok(), "Should handle cache key: {:?}", key);
        
        let retrieve_result = cache_manager.get(&key).await;
        assert!(retrieve_result.is_ok(), "Should retrieve cache key: {:?}", key);
        
        // éªŒè¯é”®çš„å­—ç¬¦ä¸²è¡¨ç¤º
        let key_string = key.to_string();
        assert!(!key_string.is_empty(), "Key string should not be empty");
        assert!(key_string.contains(src_lang), "Key string should contain source language");
        assert!(key_string.contains(target_lang), "Key string should contain target language");
    }
    
    println!("âœ… Cache key handling test passed - handled {} different key types", 
             test_cases.len());
}

/// æµ‹è¯•ç¼“å­˜é”™è¯¯å¤„ç†å’Œæ¢å¤
#[tokio::test]
async fn test_cache_error_handling_and_recovery() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 10,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // æµ‹è¯•å¤§é‡æ•°æ®çš„å¤„ç†
    let large_text = "A".repeat(100000); // 100KBæ–‡æœ¬
    let large_key = CacheKey::new(
        large_text.clone(),
        "en".to_string(),
        "zh".to_string(),
    );
    
    let large_entry = CacheEntry::new(
        large_text.clone(),
        format!("translated_{}", &large_text[..100]), // æˆªæ–­ç¿»è¯‘ä»¥èŠ‚çœå†…å­˜
        Some(Duration::from_secs(300)),
    );
    
    // å¤§æ–‡æœ¬å­˜å‚¨åº”è¯¥æˆåŠŸ
    let large_store_result = cache_manager.put(large_key.clone(), large_entry).await;
    assert!(large_store_result.is_ok(), "Should handle large text gracefully");
    
    // å¤§æ–‡æœ¬æ£€ç´¢åº”è¯¥æˆåŠŸ
    let large_retrieve_result = cache_manager.get(&large_key).await;
    assert!(large_retrieve_result.is_ok(), "Should retrieve large text gracefully");
    
    // æµ‹è¯•å¹¶å‘è®¿é—®åŒä¸€é”®
    let shared_key = CacheKey::new(
        "shared_item".to_string(),
        "en".to_string(),
        "zh".to_string(),
    );
    
    let mut concurrent_handles = vec![];
    
    for i in 0..10 {
        let cm = cache_manager.clone();
        let key = shared_key.clone();
        
        let handle = tokio::spawn(async move {
            let entry = CacheEntry::new(
                "shared_item".to_string(),
                format!("translation_{}", i),
                Some(Duration::from_secs(300)),
            );
            
            // å¹¶å‘å­˜å‚¨åŒä¸€é”®
            cm.put(key.clone(), entry).await.unwrap();
            
            // å¹¶å‘æ£€ç´¢
            let retrieved = cm.get(&key).await.unwrap();
            retrieved.is_some()
        });
        
        concurrent_handles.push(handle);
    }
    
    // ç­‰å¾…æ‰€æœ‰å¹¶å‘æ“ä½œå®Œæˆ
    let mut success_count = 0;
    for handle in concurrent_handles {
        let success = handle.await.expect("Concurrent operation should complete");
        if success {
            success_count += 1;
        }
    }
    
    assert_eq!(success_count, 10, "All concurrent operations on same key should succeed");
    
    // éªŒè¯æœ€ç»ˆçŠ¶æ€ä¸€è‡´æ€§
    let final_result = cache_manager.get(&shared_key).await.unwrap();
    assert!(final_result.is_some(), "Shared key should have a final consistent value");
    
    println!("âœ… Cache error handling test passed - handled edge cases and concurrency gracefully");
}

/// æ€§èƒ½å‹åŠ›æµ‹è¯•
#[tokio::test]
async fn test_cache_performance_stress() {
    let config = CacheConfig {
        enable_local_cache: true,
        local_cache_size: 1000,
        default_ttl: Duration::from_secs(300),
        enable_warmup: false,
        cleanup_interval: Duration::from_secs(60),
    };
    
    let cache_manager = CacheManager::new(config);
    
    // å¤§é‡æ“ä½œæ€§èƒ½æµ‹è¯•
    let num_operations = 5000;
    let test_entries = TestDataGenerator::create_test_cache_entries(num_operations);
    
    // æµ‹è¯•æ‰¹é‡å†™å…¥æ€§èƒ½
    let (_, write_time) = PerformanceHelper::measure_async_time(|| async {
        for (key, entry) in &test_entries {
            cache_manager.put(key.clone(), entry.clone()).await.unwrap();
        }
    }).await;
    
    let write_ops_per_sec = num_operations as f64 / write_time.as_secs_f64();
    
    // æµ‹è¯•æ‰¹é‡è¯»å–æ€§èƒ½
    let (_, read_time) = PerformanceHelper::measure_async_time(|| async {
        for (key, _) in &test_entries {
            let _result = cache_manager.get(key).await.unwrap();
        }
    }).await;
    
    let read_ops_per_sec = num_operations as f64 / read_time.as_secs_f64();
    
    // æ€§èƒ½æ–­è¨€
    assert!(write_ops_per_sec > 1000.0, 
           "Write performance should exceed 1000 ops/sec, got {:.1}", write_ops_per_sec);
    assert!(read_ops_per_sec > 5000.0, 
           "Read performance should exceed 5000 ops/sec, got {:.1}", read_ops_per_sec);
    
    // å†…å­˜ä½¿ç”¨æ£€æŸ¥
    let final_stats = cache_manager.get_stats();
    assert!(final_stats.total_entries > 0, "Cache should contain entries after stress test");
    
    println!("âœ… Cache performance stress test passed - Write: {:.1} ops/sec, Read: {:.1} ops/sec", 
             write_ops_per_sec, read_ops_per_sec);
}